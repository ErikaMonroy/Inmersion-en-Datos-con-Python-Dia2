
# INMERSI√ìN DE DATOS CON PYTHON üìä

Este proyecto se enfoca en el preprocesamiento y exploraci√≥n de datos de un conjunto de datos de cr√©dito alem√°n. La finalidad es construir un modelo de machine learning para evaluar la probabilidad de incumplimiento crediticio.



## Problema de Negocio üìâ

La importancia de reducir el riesgo crediticio ha llevado a una instituci√≥n financiera alemana a buscar soluciones innovadoras. Como cient√≠ficos de datos, hemos sido convocados para construir un modelo de machine learning preciso y confiable que sea capaz de evaluar con mayor precisi√≥n la probabilidad de incumplimiento crediticio de sus clientes.
# Tareas Principales üíª

- Preprocesamiento de Datos: Realizar limpieza de datos, manejar valores faltantes, codificaci√≥n de variables categ√≥ricas y normalizaci√≥n/escalado de datos.
- Exploraci√≥n de Datos: Analizar y comprender el conjunto de datos proporcionado, identificar variables clave y realizar visualizaciones para entender las relaciones entre las variables y seleccionar las caracter√≠sticas relevantes.
- Construcci√≥n de Modelos: Experimentar con algunos algoritmos de machine learning como Regresi√≥n Log√≠stica, √Årboles de Decisi√≥n, Random Forest, Naive Bayes, entre otros.
- Evaluaci√≥n y Selecci√≥n del Modelo: Evaluar los modelos utilizando m√©tricas como precisi√≥n, recall, √°rea bajo la curva ROC y F1-score. Seleccionar el modelo con el mejor rendimiento para la predicci√≥n de la solvencia crediticia.
## Desarrollo üë©üèª‚Äçüíª

El desarrollo del proyecto se llev√≥ a cabo en varias etapas. 

**1. Configuraci√≥n del Ambiente**

Primero, se configur√≥ el ambiente de trabajo importando las bibliotecas necesarias para la manipulaci√≥n y visualizaci√≥n de datos, as√≠ como para la construcci√≥n y evaluaci√≥n de modelos de machine learning. Adem√°s, se mont√≥ Google Drive para acceder al conjunto de datos.

```import pandas as pd```

```import matplotlib.pyplot as plt```

```import seaborn as sns```

```import warnings```

```from google.colab import drive```

```from sklearn.model_selection import train_test_split```

```from sklearn.linear_model import LogisticRegression```

```from sklearn.preprocessing import StandardScaler```

```from sklearn.metrics import accuracy_score, confusion_matrix, classification_report```

```from imblearn.over_sampling import SMOTE```

```from sklearn.feature_selection import SelectKBest, f_classif```

```import joblib```

**2. Preprocesamiento de Datos**

En esta etapa, se realiz√≥ la limpieza del conjunto de datos eliminando duplicados y manejando valores faltantes. Se codificaron las variables categ√≥ricas en valores num√©ricos para poder ser utilizadas en los modelos de machine learning. Este paso es crucial para asegurar que los datos est√©n en un formato adecuado para el an√°lisis y modelado.

**3. Exploraci√≥n de Datos**

Se llev√≥ a cabo un an√°lisis exploratorio de los datos para comprender mejor su distribuci√≥n y las relaciones entre las variables. Se crearon visualizaciones, como histogramas, para identificar patrones y posibles valores at√≠picos en las variables. Este an√°lisis ayud√≥ a seleccionar las caracter√≠sticas m√°s relevantes para la construcci√≥n del modelo.

**4. Construcci√≥n de Modelos**

Se entren√≥ un modelo inicial de Regresi√≥n Log√≠stica utilizando el conjunto de datos preprocesado. Este modelo se seleccion√≥ por su simplicidad y eficacia en problemas de clasificaci√≥n binaria. Se evalu√≥ el modelo inicial mediante la matriz de confusi√≥n y el reporte de clasificaci√≥n, que proporcionaron m√©tricas como la precisi√≥n, el recall y el F1-score.

**5. Evaluaci√≥n y Selecci√≥n del Modelo**
Para mejorar el rendimiento del modelo, se abordaron varios **desaf√≠os**:

- Evaluar la Matriz de Confusi√≥n: Se compararon las matrices de confusi√≥n para diferentes enfoques para entender mejor los errores de clasificaci√≥n.
- Balancear la Variable Target: Se utiliz√≥ la t√©cnica SMOTE (Synthetic Minority Over-sampling Technique) para balancear las clases en la variable target, reduciendo el sesgo hacia la clase mayoritaria.
- Seleccionar Solo Algunas Variables y Reevaluar: Se emple√≥ SelectKBest para seleccionar las caracter√≠sticas m√°s importantes, lo que ayud√≥ a mejorar la precisi√≥n del modelo eliminando caracter√≠sticas irrelevantes o redundantes.

# Visualizaci√≥n de las Matrices de Confusi√≥n

Finalmente, se visualizaron las matrices de confusi√≥n para cada uno de los enfoques (modelo original, modelo balanceado y modelo con selecci√≥n de caracter√≠sticas) para comparar su rendimiento. Estas visualizaciones permitieron identificar claramente c√≥mo cada modificaci√≥n afect√≥ la capacidad del modelo para clasificar correctamente las instancias.

## Conclusiones del Proyecto üìä

En esta segunda clase, logramos:

- Regresi√≥n Log√≠stica: Fue seleccionada debido a su alto AUC-ROC y balance razonable entre precisi√≥n y recall.
- Balanceo de Datos: El uso de SMOTE mejor√≥ significativamente la capacidad del modelo para detectar las clases minoritarias.
- Selecci√≥n de Caracter√≠sticas: Utilizando SelectKBest se logr√≥ mejorar la precisi√≥n del modelo, destacando la importancia de la selecci√≥n adecuada de caracter√≠sticas en el proceso de modelado.


## Agradecimientos üëèüèª

Este proyecto fue realizado como parte del curso **Inmersi√≥n Datos con Python** impartido por **Alura Latam** (@aluralatam) en la Clase 02: Construcci√≥n, Evaluaci√≥n y Selecci√≥n del Mejor Modelo.

Quiero expresar mi gratitud a los instructores por su excelente ense√±anza y apoyo:

- **√Ålvaro** (@ahcamachod)
- **Alejandro Gamarra** (@elprofealejo.info)
- **Christian Velasco** (@christian_pva)

Gracias por proporcionar una experiencia de aprendizaje enriquecedora y por su dedicaci√≥n a la educaci√≥n en ciencia de datos.

**#InmersionDatosAlura**
